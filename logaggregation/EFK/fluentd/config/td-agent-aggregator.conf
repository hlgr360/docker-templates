####
## Source descriptions:
##

#secure input source from log shippers
<source>
	type secure_forward
	shared_key "#{ENV['FLUENTD_SHARED_KEY']}"
	self_hostname "#{ENV['FLUENTD_HOSTNAME']}"
	#cert_auto_generate yes

	ca_cert_path /fluentd/certs/ca_cert.pem
	ca_private_key_path /fluentd/certs/ca_key.pem
	ca_private_key_passphrase "#{ENV['FLUENTD_PRIVATE_KEY_PASSPHRASE']}"

	secure true

	#for authentication
	authentication yes
	<user>
		username "#{ENV['FLUENTD_SECURE_USERNAME']}"
		password "#{ENV['FLUENTD_SECURE_PASSWORD']}"
	</user>

# Enabling this will allow only known hosts & user combinations
#	allow_anonymous_source no
#	<client>
#		network 172.18.20.97
#		users akai4life
		#also can use 'host akai.hl-network.adns.de'
#	</client>
</source>

####
## Output descriptions:
##

<match internal.**>
## We will send to both elasticsearch and stdout
    type copy
    <store>
        type elasticsearch
        logstash_format true
        host elasticsearch
				port 9200
    </store>
    <store>
    #All events pushed to stdout while fluentd is running as a service will be
    #appended to the td-agent log if started as service or to the console output
		#if started with docker-compose
        type stdout
    </store>
</match>

#This is where log messages go to die :)
<match clear>
    type null
</match>

#Log events captured by the rewrite_tag filter or output plugin will cause
#the events to be re-emitted so that they can be re-matched with their new
#tag by previously enumerated filters and match nodes.
<match **>
    type rewrite_tag_filter
    rewriterule1 sourceProject ^yourProjectName$ internal.yourProjectName
    rewriterule2 message .* clear
</match>
